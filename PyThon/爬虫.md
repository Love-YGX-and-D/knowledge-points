# urllib

## 爬虫基本套路

1. 基本流程

   * 目标数据
   * 来源地址
   * 结构分析
   * 实现构思
   * 操作编码

2. 基本手段

   * 破解请求限制

     * 请求头设置；如：useragant为有限客户端
     * 控制请求频率（根据实际情况）
     * IP代理
     * 签名/加密参数从html/cookie/js分析

   * 破解登录授权

     请求带上用户cookie

   * 破解验证码

     简单的验证码可以使用识图读验证码的第三方库

3. 解析数据

   * html dom分析
     * 正则匹配，通过正确的正则表达式来匹配想要爬取的数据，如：有些数据不在html标签里，而是在script标签的js变量中
     * 使用第三方库解析html dom，例如jquery库
   * 数据字符串
     * 正则匹配（场景）
     * 转JSON/XML对象进行解析

## 爬虫涉及的模块

1. 请求
   * urllib
   * requests
2. 多线程
   * threading
3. 正则
   * re
4. json解析
   * json
5. html dom 解析
   * beautiful soup
6. lxml：xpath
7. 操作浏览器
   * selenium

## 第一个爬虫

```python
from urllib.request import urlopen

# 地址
url='http://www.baidu.com'
#发送响应
res=urlopen(url)
#读取内容
info=res.read()

print(res.getcode()) #返回http的响应码
print(res.geturl()) #返回实际数据访问的url
print(res.info()) # 返回http响应的报头
print(info.decode())
```

## request

> user-agent:服务器端获取到的浏览器标识

### 封装request，伪造浏览器

```python
from urllib.request import Request,urlopen

url='http://www.baidu.com'

headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763'
}
# 设置
req=Request(url,headers=headers)
# 查找
print(req.get_header('User-Agent'))
res=urlopen(req)
print(res.read().decode())
```

## 请求方式

### get

```python
from urllib.request import Request,urlopen
from urllib.parse import quote

arg='汉字'
print(quote(arg))

args={'wd':'汉字'}
print(urlencode(args)) #wd=%E6%B1%89%E5%AD%97  多个 自动&连接

url='http://www.baidu.com/s?wd={0}'.format(quote(arg))
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763'
}
req=Request(url,headers=headers)
res=urlopen(req)
print(res.read().decode())
```



### post

```python
from urllib.request import Request,urlopen
from urllib.parse import urlencode
url='http:www.sxt.cn/index/login/login.html'
args={
    'user':'18335219360',
    'password':'123456'
}
f_data=urlopen(args)
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763'
}
req=Request(url,headers=headers,data=f_data.encode())
res=urlopen(req)
print(res.read().decode())
```

## 爬贴吧

```python
from urllib.request import Request,urlopen
from urllib.parse import urlencode,quote
def get_html(url):
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763'
    }
    req=Request(url,headers=headers)
    res=urlopen(req)
    return res.read().decode()

def save_html(html,name):
    with open(name,'w',encoding='utf-8') as f:
        f.write(html)

def main():
    tieba=input('请输入爬取的贴吧名：')
    url='http://tieba.baidu.com/f?kw=%s&ie=utf-8&pn=s'%quote(tieba)
    num = int(input('请输入爬取的页面数'))
    for i in range(num):
        i1=i*50
        html=get_html(url+str(i1))
        save_html(html,'第%d个页面.html'%i)

if __name__ == '__main__':
    main()
```

## ajax-豆瓣

```python
from urllib.request import Request,urlopen

base_url='https://movie.douban.com/explore#!type=movie&tag=%E8%B1%86%E7%93%A3%E9%AB%98%E5%88%86&sort=recommend&page_limit=20&page_start={}'

headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763'
}

i=0
while True:
    req=Request(base_url.format(i*20),headers=headers)
    res=urlopen(req)
    print(res.read().decode())
    if res.read().decode():
        i += 1
    else:
        break
```

## https-证书

```python
from urllib.request import Request,urlopen
import ssl
url='https://www.12306.cn/mormhweb/'
headers={
    'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/64.0.3282.140 Safari/537.36 Edge/18.17763'
}
req=Request(url,headers=headers)
#忽略证书
context=ssl._create_unverified_context()
res=urlopen(req,context=context)
print(res.read().decode())
```

## 动态User-Agent的用法

```python
from urllib.request import Request,urlopen
from random import choice

url="http://www.httpbin.org/get"
UserAgent=['Mozilla/5.0 (Windows NT 10.0; Win64; x64)1','Mozilla/5.0 (Windows NT 10.0; Win64; x64)2','Mozilla/5.0 (Windows NT 10.0; Win64; x64)3']

# 第三方模块  fake-useragent
# from fake-useragent import UserAgent
# ua=UserAgent()
# ua.ie/chorme/firefox  #连续用相同的，随机生成其他版本，内核一样
# ua.random  随机获取
headers={
    'User-Agent':ua.random
    #'User-Agent':choice(UserAgent)
}
req=Request(url,headers=headers)
res=urlopen(req)
print(res.read().decode())
```

##构造urlopen

```python
from urllib.request import Request,HTTPHandler,build_opener
from fake_useragent import UserAgent

url='http://httpbin.org/get'
headers={
    'User-Agent':UserAgent().random
}
req=Request(url,headers=headers)
handle=HTTPHandler(debuglevel=1)
opener=build_opener(handle)
res=opener.open(req)
print(res.read().decode())
```

## 代理

1. 透明代理：目标网站知道使用了代理和你的源IP地址
2. 匿名代理：匿名程度低，知道你使用了代理，不知道源IP
3. 高级代理：最保险，不知道使用了代理也不知道源IP

 ```python
from urllib.request import Request,HTTPHandler,build_opener,ProxyHandler
from fake_useragent import UserAgent

url='http://httpbin.org/get'
headers={
    'User-Agent':UserAgent().random
}
req=Request(url,headers=headers)
#handle=ProxyHandler({'http':"ip:port"})
handle=ProxyHandler({'http':"219.141.153.41:80"})
#handle=ProxyHandler({'http':"name:password@ip:port"})
# handle=ProxyHandler({'http':"398707160:j8inhg2g@39.105.117.225:16818"})
opener=build_opener(handle)
res=opener.open(req)
print(res.read().decode())
 ```

## cookie

### 浏览器获取cookie

```python
from urllib.request import Request,urlopen
from fake_useragent import UserAgent
url='http://xsjwxt.sxau.edu.cn:7872/index.jsp'
headers={
    'User-Agent':UserAgent().random,
    'Cookie':'JSESSIONID=abcW1TTHeS69kbXcwmWNw'
}
req=Request(url,headers=headers)
res=urlopen(req)
print(res.read().decode())
```

### 先登录再保存cookie

```python
from urllib.request import Request,build_opener,HTTPCookieProcessor
from urllib.parse import urlencode
from fake_useragent import UserAgent

login_url='http://www.sxt.cn/index/login/login'
form_data={
    'user':'17703181473',
    'password':'123456'
}
headers={'User-Agent':UserAgent().random}
login_req=Request(login_url,headers=headers,data=urlencode(form_data).encode())
handel=HTTPCookieProcessor()
login_opener=build_opener(handel)
login_res=login_opener.open(login_req)

info_url='http://www.sxt.cn/index/user.html'
info_req=Request(info_url,headers=headers)
info_opener=build_opener(handel)
info_res=info_opener.open(info_req)
print(info_res.read().decode())
```

### 共享cookie

```python
from urllib.request import Request,build_opener,HTTPCookieProcessor
from urllib.parse import urlencode
from fake_useragent import UserAgent
from http.cookiejar import MozillaCookieJar

def get_cookie():
    login_url = 'http://www.sxt.cn/index/login/login'
    form_data = {
        'user': '17703181473',
        'password': '123456'
    }
    headers = {'User-Agent': UserAgent().random}
    login_req = Request(login_url, headers=headers, data=urlencode(form_data).encode())
    cookie_jar=MozillaCookieJar()
    handel = HTTPCookieProcessor(cookie_jar)
    login_opener = build_opener(handel)
    login_res = login_opener.open(login_req)
    cookie_jar.save('cookie.txt',ignore_discard=True,ignore_expires=True)

def use_cookie():
    headers = {'User-Agent': UserAgent().random}
    info_url = 'http://www.sxt.cn/index/user.html'
    info_req = Request(info_url, headers=headers)
    cookie_jar=MozillaCookieJar()
    cookie_jar.load('cookie.txt',ignore_expires=True,ignore_discard=True)
    handel = HTTPCookieProcessor(cookie_jar)
    info_opener = build_opener(handel)
    info_res = info_opener.open(info_req)
    print(info_res.read().decode())

if __name__ == "__main__":
    get_cookie()
    use_cookie()
```

## 捕获URLerror

```python
from urllib.request import Request,urlopen
from fake_useragent import UserAgent
from urllib.error import URLError

headers={'User-Agent':UserAgent().random}
url='http://www.sxt.cn/index/login/login'
req=Request(url,headers=headers)
try:
    res=urlopen(req)
    print(res.read().decode())
except URLError as e:
    print(e)
finally:
    print('爬取完成')
```

# requests

> pip install requests

## 基本请求

1. req=requests.get('网址')
2. req=requests.post('网址')
3. req=requests.put('网址')
4. req=requests.delete('网址')
5. req=requests.head('网址')
6. req=requests.options('网址')

## get请求

> 参数是字典方式

1. 不带参数

   ```python
   import requests
   url='http://www.baidu.com'
   req=requests.get(url)
   req.encoding='utf-8'
   print(req.text)
   ```

2. 带参数

   ```python
   import requests
   url='http://www.baidu.com/s'
   params={
       'wd':'山西农业大学'
   }
   from fake_useragent import UserAgent
   req=requests.get(url,params=params,headers={'User-Agent':UserAgent().random},timeout=0.02)
   # timeout 超时时间
   req.encoding='utf-8'
   print(req.text)
   ```

## post请求

```python
from fake_useragent import UserAgent
import requests
url='http://www.sxt.cn/index/login/login.html'
args={
    'user':'18335219360',
    'password':'123456'
}
headers={
    'User-Agent':UserAgent().random
}
req=requests.post(url,data=args,headers=headers)
req.encoding='utf-8'
print(req.text)
```

## 代理

```python
from fake_useragent import UserAgent
import requests
url='http://httpbin.org/get'
proxy={
    # "http":'http://ip:port'
    # "https":'https://ip:port'
    # "http":'http://name:password@ip:port'
    "http":'http://106.75.9.39:8080'
}
headers={
    'User-Agent':UserAgent().random
}
req=requests.post(url,proxies=proxy,headers=headers)
req.encoding='utf-8'
print(req.text)
```

## session

```python
from fake_useragent import UserAgent
import requests
login_url = 'http://www.sxt.cn/index/login/login'
form_data = {
    'user': '17703181473',
    'password': '123456'
}
headers = {'User-Agent': UserAgent().random}
session=requests.Session()
req=session.post(login_url,headers=headers,data=form_data)
req.encoding='utf-8'
print(req.text)
```

## ssl验证

```python
# 禁用安全请求警告
requests.packages.urllib3.disable_warnings()
req=requests.get(url,verify=False,headers=headers)
```

## 获取响应信息

| 代码                | 含义                         |
| ------------------- | ---------------------------- |
| res.json()          | 获取响应内容（以json字符串） |
| res.text()          | 获取响应内容（以字符串）     |
| res.content         | 获取响应内容（以字节）       |
| res.headers         | 获取响应头                   |
| res.url             | 获取响应地址                 |
| res.encoding        | 获取网页编码                 |
| res.request.headers | 获取请求头内容               |
| res.cookie          | 获取cookie                   |

# 数据提取与验证码识别

## 提取数据

### 正则

### beautiful soup * =>bs4

> 安装：`pip install bs4` `pip install beautifulsoup4 `
>
> pip install lxml

#### 常用方法

| 解析器              | 使用方法                                                     | 优势                                                        | 劣势                        |
| ------------------- | ------------------------------------------------------------ | ----------------------------------------------------------- | --------------------------- |
| Python标准库        | BeautifulSoup(要解析的代码,'html.parser')                    | 1.python的内置标准库 2. 执行速度适中 3.容错能力强           | 2和3的中文容错能力差        |
| **lxml html解析器** | BeautifulSoup(要解析的代码,'lxml')                           | 1.速度快 2.容错能力强                                       | 需要安装c语言库             |
| lxml xml解析器      | BeautifulSoup(要解析的代码,['lxml','xml']) BeautifulSoup(要解析的代码,'xml') | 1.速度快 2.唯一支持xml的解析器                              | 需要安装c语言库             |
| html5lib            | BeautifulSoup(要解析的代码,html5lib')                        | 1. 最好的容错 2. 以浏览器的方式解析文档 3. 生成H5格式的文档 | 1. 速度慢 2. 不依赖外部扩展 |

```python
from bs4 import BeautifulSoup
bs=BeautifulSoup(html,'lxml')
```

#### 四大对象种类

1. Tag ：html中的一个标签
2. NavigableString
3. BeautifulSoup
4. Comment

#### 使用方式

```html
<title>杨国璇</title>
<div class='info'>爱你</div>
<div class='info'>
    <span>好好学习</span>
    <a href='www.baidu.com'>百度</a>
    <strong>修饰</strong>
</div>
```

##### 获取标签

```python
bs.标签  =》<title>杨国璇</title>
# 搜索到的第一个标签
# 只获取文本信息bs.标签.text
```

##### 获取属性

```python
bs.标签.attrs   #获取所有，字典格式
bs.标签.get('属性名') #获取单个
bs.标签['属性名'] #获取单个
```

##### 获取内容

```python
bs.标签.string
bs.标签.text 
```

##### 注释信息（返回Comment对象）

```python
bs4.element对象下的Comment
bs.strong.prettify()=》原样打印
bs.strong.string
```

##### 筛选多个标签/按条件筛选

```python
bs.fiand_all('标签')
bs.fiand_all(['span','a'])
bs.fiand_all(属性名='值')
bs.fiand_all('标签',class_='info') # class特殊
bs.fiand_all('标签',attr={'class':'info'})
```

##### css选择器

```python
bs.select('css选择器')
```

### xPath *

#### 常用的路劲表达式

| 表达式   | 描述                                                     |
| -------- | -------------------------------------------------------- |
| nodename | 选取此节点的所有子节点                                   |
| /        | 从根节点选取                                             |
| //       | 从匹配选择的当前节点选择文档中的节点，而不考虑他们的位置 |
| .        | 选取当前节点                                             |
| ..       | 选取当前节点的父节点                                     |
| @        | 选取属性                                                 |

#### 通配符

> 选取未知的xml元素

| 通配符 | 描述               | 举例             | 结果                    |
| ------ | ------------------ | ---------------- | ----------------------- |
| *      | 匹配任何元素节点   | xpath('div/*')   | 获取div下的所有子节点   |
| @*     | 匹配任何属性节点   | xpath('div[@*]') | 获取所有带属性的div节点 |
| node() | 匹配任何类型的节点 |                  |                         |

#### 多个

```python
xpath('//div | //table') 
# 获取所有的div与table节点
```

#### 谓语

| 表达式                            | 结果                            |
| --------------------------------- | ------------------------------- |
| xpath('/body/div[1]')             | 选取body下的第一个div节点       |
| xpath('/body/div[last()]')        | 选取body下的最后一个div节点     |
| xpath('/body/div[last()-1]')      | 选取倒数第二个                  |
| xpath('/body/div[posion()<3]')    | 前两个                          |
| xpath('/body/div[@class]')        | 带有class属性的div节点          |
| xpath('/body/div[@class="main"]') | 选取class属性值为main的div节点  |
| xpath('/body/div[price]>35.00')   | 选取body下price元素大于35的节点 |

#### 运算符

`+ - * div = != < <= > >= or and mod`

#### 获取属性值

`//div[@id="indexCarousel"]//div[@class="item"]//img/@src`

`//div[@class="book-mid-info"]/h4/a/@href`

### PyQuery

> `pip install puquery`

#### 初始化

1. 字符串 **常用**

   ```python
   from pyquery import PyQuery as pq
   doc=pq(str)
   print(doc(tagname))
   ```

2. url

   ```python
   from pyquery import PyQuery as pq
   doc=pq(url='http://www.baidu.com')
   print(doc('title'))
   ```

3. 文件

   ```python
   from pyquery import PyQuery as pq
   doc=pq(filename='demo.html')
   print(doc(tagname))
   ```

#### 选择节点

1. 选择当前节点

   ```python
   from pyquery import PyQuery as pq
   doc=pq(str)
   doc('#main #top')
   ```

2. 选取子节点

   * 在doc中一层层写出来
   * 获取到父元素使用children方法

   ```python
   from pyquery import PyQuery as pq
   doc=pq(str)
   doc('#main #top').children()
   ```

3. 获取父节点

   > 获取带当前节点后使用parent方法

   ```python
   doc('#main #top').parent()
   ```

4. 获取兄弟节点

   > 获取到当前节点后使用siblings方法

   ```python
   doc('#main #top').siblings()
   ```

5. 第n个节点

   ```python
   .eq(n)
   ```


#### 获取属性

```python
from pyquery import PyQuery as pq
doc=pq(str)
aa=doc('#main #top')
aa.attrib['属性名']
pq对象用attr
```

#### 获取内容

```python
from pyquery import PyQuery as pq
doc=pq(str)
aa=doc('#main #top').children()
aa.html()
aa.text()
```

### jsonPath

> pip install jsonpath

#### jsonpath与xpath的对比

| xPath | jsonpath | 描述                         |
| ----- | -------- | ---------------------------- |
| /     | $        | 根节点                       |
| .     | @        | 当前节点                     |
| /     | .or[]    | 子节点                       |
| ..    |          | 父节点,jsonpath不支持        |
| //    | ..       | 不管位置，筛选所有符合的节点 |
| *     | *        | 所有的元素节点               |
| @     |          | 根据属性访问，jsonpath不支持 |
| []    | []       | 迭代器标示                   |
|       | [,]      | 迭代器中的多选               |
| []    | ?()      | 过滤操作                     |
|       | ()       | 表达式计算                   |
| ()    |          | 分组，jsonpath不支持         |

## 验证码Tesseract

> tesseract 是一个google支持的开源ocr项目
>
> 其项目地址：https://github.com/tesseract-ocr/tesseract
>
> 目前最新的源码可以在这里下载
>
> 图片识别=》图片文字转换成字符串
>
> <http://www.icecast.org/download/>

`http://3.onj.me/tesseract/`

```python

Tesseract for Windows二进制文件
请注意：Tesseract的编译版本是前沿代码。所以这可能会被打破或部分起作用。

大家好，这是一个如何在您的计算机上安装Tesseract的简短说明。

这实际上非常简单：
有1个exe文件：
正方体，yyyymmdd.exe
具有所有语言数据的Tesseract应用程序。
yyyymmdd表示年份4位数，月份2位数字和第2天数字。
该应用程序是便携式的，因此您可以将其安装在USB记忆棒或其他位置。

安装步骤：
1.下载tesseract包。
2.双击tesseract包并将其解压缩到您希望的目录。
3.为方便起见，您可以将tesseract.exe的路径添加到PATH变量，并通过执行以下操作添加TESSDATA_PREFIX变量：
*您必须拥有管理员权限。
*移动到桌面。
*右键单击计算机或此计算机。
*选择属性
*在以下对话框中单击高级系统设置链接（Windows 7/8/10）
*单击环境变量按钮。
* tab到系统变量列表。
*选择PATH变量并单击编辑。
*在win 7上，您将获得直接编辑框，您可以在移动到结尾后输入路径并在最后一个路径中添加分号。在win 10上，您只需单击new并输入新路径。
*进入路径后输入。
*单击“新建”以创建新的系统变量。
*为新变量和命中选项卡输入名称TESSDATA_PREFIX。 
*输入tessdata目录的路径。经常路径\到\ tesseract \ tessdata。
*在弹出的所有对话框中单击“确定”。
如果您没有管理员权限，则需要从其主要位置调用tesseract。这意味着exe文件的位置。

删除你只需要删除tesseract目录并清理之前修改过的变量。
打开命令提示符并导航到安装路径。
对于测试，您可以输入：
tesseract filename.tif filename -l eng pdf
这将生成filename.txt和filename.pdf文件。
PDF文件是可搜索的。

请注意：对于盲人来说，这个PDF版本目前有点破，你无法阅读所有文字。使用普通文本输出。

您还可以使用以下命令创建一个hOCR文件，它是一种HTML文件：
tesseract filename.tif filename -l eng hocr

你可以通过这样做来创建一个文本文件：
tesseract filename.tif filename -l eng

在tessdata目录中，您可以找到各种语言。
-l标志激活语言。
deu为德语，eng为英语或rus为俄语或ukr为乌克兰语等等。
请查看tessdata目录以获取更多语言或只需输入
tesseract --list-langs

您还可以添加更多语言来检查多语言文本。
tesseract filename.tif filename -l eng + deu
这将激活英语和德语语言模块。

doc目录包含一些文档。
你也可以进入
tesseract --help寻求帮助。

在测试目录中，您可以找到一些测试图像。
Tesseract目前支持多页TIF文件，JPG，WEBP文件等。
它不支持多页PDF作为输入。
虽然您可以使用ImageMagick将多页PDF转换为多页TIF文件。

有更新时，您可以将文件解压缩到目录中，并用新文件替换旧文件。
只需按照上面的步骤。
Tesseract源代码和语言数据可用
https://github.com/tesseract-ocr/

```

### 使用

```python
tesseract 要识别的图片 输出的文件
```

####  Tesseract ocr使用

安装之后，默认目录C:\Program Files (x86)\Tesseract-OCR，你需要把这个路径放到你操作系统的path搜索路径中，否则后面使用起来会不方便。

在安装目录C:\Program Files (x86)\Tesseract-OCR下可以看到 tesseract.exe这个命令行执行程序

```
tesseract 1.png output-l eng -psm 7
```

-psm 7 表示用单行文本识别
pagesegmode值：

- 0 =定向和脚本检测（OSD）。
- 1 =带OSD的自动页面分割。
- 2 =自动页面分割，但没有OSD或OCR
- 3 =全自动页面分割，但没有OSD。（默认）
- 4 =假设一列可变大小的文本。
- 5 =假设一个统一的垂直对齐文本块。
- 6 =假设一个统一的文本块。
- 7 =将图像作为单个文本行处理。
- 8 =把图像当作一个单词。
- 9 =把图像当作一个圆圈中的一个词来对待。
- 10 =将图像作为单个字符处理

 ###-l eng 代表使用英语识别

#### 参考

www.cnblogs.com/wzben/p/5930538.html

## pytesseract

```python
import pytesseract
from PIL import Image
img=Image.open('yzm2.jpeg')
str=pytesseract.image_to_string(img)
print(str)
```

## Selenium

Selenium是一个Web的自动化测试工具，最初是为网站自动化测试而开发的，类型像我们玩游戏用的按键精灵，可以按指定的命令自动操作，不同是Selenium 可以直接运行在浏览器上，它支持所有主流的浏览器（包括PhantomJS这些无界面的浏览器）。

Selenium 可以根据我们的指令，让浏览器自动加载页面，获取需要的数据，甚至页面截屏，或者判断网站上某些动作是否发生。

Selenium 自己不带浏览器，不支持浏览器的功能，它需要与第三方浏览器结合在一起才能使用。但是我们有时候需要让它内嵌在代码中运行，所以我们可以用一个叫 PhantomJS 的工具代替真实的浏览器。

PyPI网站下载 Selenium库 https://pypi.python.org/simple/selenium ，也可以用 第三方管理器 

pip用命令安装：`pip install selenium`

Selenium 官方参考文档：http://selenium-python.readthedocs.io/index.html

### PhantomJS

PhantomJS 是一个基于Webkit的“无界面”(headless)浏览器，它会把网站加载到内存并执行页面上的 JavaScript，因为不会展示图形界面，所以运行起来比完整的浏览器要高效

如果我们把 Selenium 和 PhantomJS 结合在一起，就可以运行一个非常强大的网络爬虫了，这个爬虫可以处理 JavaScrip、Cookie、headers，以及任何我们真实用户需要做的事情

#### 注意：PhantomJS（python2）

只能从它的官方网站http://phantomjs.org/download.html) 下载。 因为 PhantomJS 是一个功能完善(虽然无界面)的浏览器而非一个 Python 库，所以它不需要像 Python 的其他库一样安装，但我们可以通过Selenium调用PhantomJS来直接使用。

PhantomJS 官方参考文档：http://phantomjs.org/documentation

####  python3使用的浏览器

随着Python3的普及，Selenium3也跟上了行程。而Selenium3最大的变化是去掉了Selenium RC，另外就是Webdriver从各自浏览器中脱离，必须单独下载

##### 安装Firefox geckodriver

安装firefox最新版本，添加Firefox可执行程序到系统环境变量。记得关闭firefox的自动更新

firefox下载地下：https://github.com/mozilla/geckodriver/releases

将下载的geckodriver.exe 放到path路径下 D:\Python\Python36\

##### 安装ChromeDriver

http://chromedriver.storage.googleapis.com/index.html

> 注意版本号要对应

> 下载下来的文件解压到`Python36\Scripts`

> chrome59版本以后可以变成无头的浏览器，加以下参数

```
options = webdriver.ChromeOptions()
options.add_argument('--headless')
chrome = webdriver.Chrome(chrome_options=options)
chrome.get("http://ww.baidu.com")
```

> 代理模式

```
from selenium import webdriver
option = webdriver.ChromeOptions()
option.add_argument("--proxy-server=http://61.138.33.20:808")
chrome = webdriver.Chrome(chrome_options=option)
chrome.get('http://httpbin.org/get')
info = chrome.page_source

print(info)

```

###  使用方式

Selenium 库里有个叫 WebDriver 的 API。WebDriver 有点儿像可以加载网站的浏览器，但是它也可以像 BeautifulSoup 或者其他 Selector 对象一样用来查找页面元素，与页面上的元素进行交互 (发送文本、点击等)，以及执行其他动作来运行网络爬虫

#### 简单例子

```
# 导入 webdriver
from selenium import webdriver

# 要想调用键盘按键操作需要引入keys包
from selenium.webdriver.common.keys import Keys

# 调用环境变量指定的PhantomJS浏览器创建浏览器对象
driver = webdriver.PhantomJS()

# 如果没有在环境变量指定PhantomJS位置
# driver = webdriver.PhantomJS(executable_path="./phantomjs"))

# get方法会一直等到页面被完全加载，然后才会继续程序，通常测试会在这里选择 time.sleep(2)
driver.get("http://www.baidu.com/")

# 获取页面名为 wrapper的id标签的文本内容
data = driver.find_element_by_id("wrapper").text

# 打印数据内容
print(data)

# 打印页面标题 "百度一下，你就知道"
print（driver.title）

# 生成当前页面快照并保存
driver.save_screenshot("baidu.png")

# id="kw"是百度搜索输入框，输入字符串"长城"
driver.find_element_by_id("kw").send_keys("ygx")

# id="su"是百度搜索按钮，click() 是模拟点击
driver.find_element_by_id("su").click()

# 获取新的页面快照
driver.save_screenshot("ygx.png")

# 打印网页渲染后的源代码
print(driver.page_source)

# 获取当前页面Cookie
print(driver.get_cookies())

# ctrl+a 全选输入框内容
driver.find_element_by_id("kw").send_keys(Keys.CONTROL,'a')

# ctrl+x 剪切输入框内容
driver.find_element_by_id("kw").send_keys(Keys.CONTROL,'x')

# 输入框重新输入内容
driver.find_element_by_id("kw").send_keys("python爬虫")

# 模拟Enter回车键
driver.find_element_by_id("su").send_keys(Keys.RETURN)

# 清除输入框内容
driver.find_element_by_id("kw").clear()

# 生成新的页面快照
driver.save_screenshot("python爬虫.png")

# 获取当前url
print(driver.current_url)

# 关闭当前页面，如果只有一个页面，会关闭浏览器
# driver.close()

# 关闭浏览器
driver.quit()
```

### 页面操作

#### 页面交互

> 仅仅抓取页面没有多大卵用，我们真正要做的是做到和页面交互，比如点击，输入等等。那么前提就是要找到页面中的元素。WebDriver提供了各种方法来寻找元素。例如下面有一个表单输入框

```
<input type="text" name="passwd" id="passwd-id" />

```

##### **获取**

```
element = driver.find_element_by_id("passwd-id")
element = driver.find_element_by_name("passwd")
element = driver.find_elements_by_tag_name("input")
element = driver.find_element_by_xpath("//input[@id='passwd-id']")
```

**注意：**

- 文本必须完全匹配才可以，所以这并不是一个很好的匹配方式
- 在用 xpath 的时候还需要注意的如果有多个元素匹配了 xpath，它只会返回第一个匹配的元素。如果没有找到，那么会抛出 NoSuchElementException 的异常

##### 输入内容

```
element.send_keys("some text")
```

##### 模拟点击某个按键

```
element.send_keys("and some", Keys.ARROW_DOWN)
```

#####  清空文本

```
element.clear()
```

##### 元素拖拽

> 要完成元素的拖拽，首先你需要指定被拖动的元素和拖动目标元素，然后利用 ActionChains 类来实现

以下实现元素从 source 拖动到 target 的操作

```
element = driver.find_element_by_name("source")
target = driver.find_element_by_name("target")
 
from selenium.webdriver import ActionChains
action_chains = ActionChains(driver)
action_chains.drag_and_drop(element, target).perform()
```

##### 历史记录

> 操作页面的前进和后退功能

```
driver.forward()
driver.back()
```

### API

#### 元素选取

##### 单个元素选取

- find_element_by_id
- find_element_by_name
- find_element_by_xpath
- find_element_by_link_text
- find_element_by_partial_link_text
- find_element_by_tag_name
- find_element_by_class_name
- find_element_by_css_selector

#####  多个元素选取

- find_elements_by_name
- find_elements_by_xpath
- find_elements_by_link_text
- find_elements_by_partial_link_text
- find_elements_by_tag_name
- find_elements_by_class_name
- find_elements_by_css_selector

##### 利用 By 类来确定哪种选择方式

```
from selenium.webdriver.common.by import By
 
driver.find_element(By.XPATH, '//button[text()="Some text"]')
driver.find_elements(By.XPATH, '//button')
```

By 类的一些属性如下

- ID = "id"
- XPATH = "xpath"
- LINK_TEXT = "link text"
- PARTIAL_LINK_TEXT = "partial link text"
- NAME = "name"
- TAG_NAME = "tag name"
- CLASS_NAME = "class name"
- CSS_SELECTOR = "css selector"

### 等待

####  隐式等待

> 到了一定的时间发现元素还没有加载，则继续等待我们指定的时间，如果超过了我们指定的时间还没有加载就会抛出异常，如果没有需要等待的时候就已经加载完毕就会立即执行

```
from selenium import webdriver
url = 'https://www.guazi.com/nj/buy/'
driver = webdriver.Chrome()
driver.get(url)
driver.implicitly_wait(100)
print(driver.find_element_by_class_name('next'))
print(driver.page_source)
```

#### 显示等待

> 指定一个等待条件，并且指定一个最长等待时间，会在这个时间内进行判断是否满足等待条件，如果成立就会立即返回，如果不成立，就会一直等待，直到等待你指定的最长等待时间，如果还是不满足，就会抛出异常，如果满足了就会正常返回

```
    url = 'https://www.guazi.com/nj/buy/'
    driver = webdriver.Chrome()
    driver.get(url)
    wait = WebDriverWait(driver,10)
    wait.until(EC.presence_of_element_located((By.CLASS_NAME, 'next')))
    print(driver.page_source)
```

- presence_of_element_located   
  - 元素加载出，传入定位元组，如(By.ID, 'p')
- presence_of_all_elements_located 
  - 所有元素加载出
- element_to_be_clickable
  - 元素可点击
- element_located_to_be_selected
  - 元素可选择，传入定位元组 

#### 强制等待

> 使用 time.sleep

### Selenium 处理滚动条

> selenium并不是万能的，有时候页面上操作无法实现的，这时候就需要借助JS来完成了

　　当页面上的元素超过一屏后，想操作屏幕下方的元素，是不能直接定位到，会报元素不可见的。这时候需要借助滚动条来拖动屏幕，使被操作的元素显示在当前的屏幕上。滚动条是无法直接用定位工具来定位的。selenium里面也没有直接的方法去控制滚动条，这时候只能借助J了，还好selenium提供了一个操作js的方法:execute_script()，可以直接执行js的脚本

#### 一. 控制滚动条高度

###### 1.1滚动条回到顶部：

```
js="var q=document.getElementById('id').scrollTop=0"
driver.execute_script(js)
```

###### 1.2滚动条拉到底部

```
js="var q=document.documentElement.scrollTop=10000"
driver.execute_script(js)
```

可以修改scrollTop 的值，来定位右侧滚动条的位置，0是最上面，10000是最底部

以上方法在Firefox和IE浏览器上上是可以的，但是用Chrome浏览器，发现不管用。Chrome浏览器解决办法：

```
js = "var q=document.body.scrollTop=0"
driver.execute_script(js)
```

 

#### 二.横向滚动条

###### 2.1 有时候浏览器页面需要左右滚动（一般屏幕最大化后，左右滚动的情况已经很少见了)

###### 2.2 通过左边控制横向和纵向滚动条scrollTo(x, y)

```
js = "window.scrollTo(100,400)"
driver.execute_script(js)
```

#### 三.元素聚焦

虽然用上面的方法可以解决拖动滚动条的位置问题，但是有时候无法确定我需要操作的元素在什么位置，有可能每次打开的页面不一样，元素所在的位置也不一样，怎么办呢？这个时候我们可以先让页面直接跳到元素出现的位置，然后就可以操作了

同样需要借助JS去实现。 具体如下：

```
target = driver.find_element_by_xxxx()
driver.execute_script("arguments[0].scrollIntoView();", target)
```

#### 四. 参考代码

```python
from selenium import webdriver
from lxml import etree
import time

url = "https://search.jd.com/Search?keyword=%E7%AC%94%E8%AE%B0%E6%9C%AC&enc=utf-8&wq=%E7%AC%94%E8%AE%B0%E6%9C%AC&pvid=845d019c94f6476ca5c4ffc24df6865a"
# 加载浏览器
wd = webdriver.Firefox()
# 发送请求
wd.get(url)
# 要执行的js
js = "var q = document.documentElement.scrollTop=10000"
# 执行js
wd.execute_script(js)

time.sleep(3)
# 解析数据
e = etree.HTML(wd.page_source)
# 提取数据的xpath
price_xpath = '//ul[@class="gl-warp clearfix"]//div[@class="p-price"]/strong/i/text()'
# 提取数据的
infos = e.xpath(price_xpath)

print(len(infos))
# 关闭浏览器
wd.quit()
```

# Scrapy

1. 特点
   * 开源免费
   * 生成格式：json、csv、xml
   * 支持xPath和css表达式
   * 允许自动从网页爬数据
2. 优点
   * 易扩展，快速和功能强大
   * 可移植性强
   * Scrapy请求调度和异步处理
   * 附带了名为Scrapyd的内置服务器。允许使用json web服务上传项目和控制蜘蛛
   * 能够刮削任何网站，即使网站不具有原始数据访问API

3. 安装scrapy

   ```python
   sudo apt-get install libxml2-dev libxslt1-dev python-dev
   sudo apt-get install zlib1g-dev
   sudo apt-get install libevent-dev
   sudo pip install lxml
   
   ```


## 创建应用

> `scrapy startproject firstproject（）`

## 目录及内容解释

1. scrapy.cfg : 项目的总配置文件

   ```python
   # Automatically created by: scrapy startproject
   #
   # For more information about the [deploy] section see:
   # https://scrapyd.readthedocs.io/en/latest/deploy.html
   
   [settings]
   default = firstproject.settings  #具体的项目配置文件
   
   [deploy]
   #url = http://localhost:6800/   #部署的地址
   project = firstproject #部署的项目名称
   
   ```

2. firstproject：项目

   * spiders：创建爬虫文件

     * 创建爬虫文件的方法

       ```python
       scrapy genspider example example.com
       ```

       > 在其文件下生成`example.py`文件

       ```python
       # -*- coding: utf-8 -*-
       import scrapy
       
       class BaiduSpider(scrapy.Spider):
           name = 'baidu'  #爬虫名称；唯一的
           allowed_domains = ['baidu.com']  #允许爬取的域名
           start_urls = ['http://www.baidu.com/'] #修改为正确的开始url
       
           def parse(self, response):
               # 响应回来的数据
               print(response.text)
       ```

     * 启动爬虫

       - `scrapy crawl example`

       - 在项目下创建启动文件`start.py`

         ```python
         from scrapy.cmdline import execute
         execute('scrapy crawl example'.split()) #接收列表
         ```

   * items.py

     ```python
     # -*- coding: utf-8 -*-
     
     # Define here the models for your scraped items
     # See documentation in:
     # https://doc.scrapy.org/en/latest/topics/items.html
     
     import scrapy
     # 相当于django的model
     class FirstprojectItem(scrapy.Item):
         # define the fields for your item here like:
         # name = scrapy.Field()
         pass
     ```

   * middlewares.py ：处理下载之前或者下载之后的事情

     ```python
     # -*- coding: utf-8 -*-
     
     # Define here the models for your spider middleware
     # See documentation in:
     # https://doc.scrapy.org/en/latest/topics/spider-middleware.html
     
     from scrapy import signals
     
     class FirstprojectSpiderMiddleware(object):
         # Not all methods need to be defined. If a method is not defined,
         # scrapy acts as if the spider middleware does not modify the
         # passed objects.
     
         @classmethod
         def from_crawler(cls, crawler):
             # This method is used by Scrapy to create your spiders.
             s = cls()
             crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
             return s
     
         def process_spider_input(self, response, spider):
             # Called for each response that goes through the spider
             # middleware and into the spider.
     
             # Should return None or raise an exception.
             return None
     
         def process_spider_output(self, response, result, spider):
             # Called with the results returned from the Spider, after
             # it has processed the response.
     
             # Must return an iterable of Request, dict or Item objects.
             for i in result:
                 yield i
     
         def process_spider_exception(self, response, exception, spider):
             # Called when a spider or process_spider_input() method
             # (from other spider middleware) raises an exception.
             # Should return either None or an iterable of Response, dict
             # or Item objects.
             pass
     
         def process_start_requests(self, start_requests, spider):
             # Called with the start requests of the spider, and works
             # similarly to the process_spider_output() method, except
             # that it doesn’t have a response associated.
             # Must return only requests (not items).
             for r in start_requests:
                 yield r
     
         def spider_opened(self, spider):
             spider.logger.info('Spider opened: %s' % spider.name)
     
     class FirstprojectDownloaderMiddleware(object):
         # Not all methods need to be defined. If a method is not defined,
         # scrapy acts as if the downloader middleware does not modify the
         # passed objects.
         
         @classmethod
         def from_crawler(cls, crawler):
             # This method is used by Scrapy to create your spiders.
             s = cls()
             crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
             return s
     
         def process_request(self, request, spider):
             # Called for each request that goes through the downloader
             # middleware.
     
             # Must either:
             # - return None: continue processing this request
             # - or return a Response object
             # - or return a Request object
             # - or raise IgnoreRequest: process_exception() methods of
             #   installed downloader middleware will be called
             return None
     
         def process_response(self, request, response, spider):
             # Called with the response returned from the downloader.
     
             # Must either;
             # - return a Response object
             # - return a Request object
             # - or raise IgnoreRequest
             return response
     
         def process_exception(self, request, exception, spider):
             # Called when a download handler or a process_request()
             # (from other downloader middleware) raises an exception.
     
             # Must either:
             # - return None: continue processing this exception
             # - return a Response object: stops process_exception() chain
             # - return a Request object: stops process_exception() chain
             pass
     
         def spider_opened(self, spider):
             spider.logger.info('Spider opened: %s' % spider.name)
     ```

   * pipelines.py ：保存数据

     ```python
     # -*- coding: utf-8 -*-
     
     # Define your item pipelines here
     # Don't forget to add your pipeline to the ITEM_PIPELINES setting
     # See: https://doc.scrapy.org/en/latest/topics/item-pipeline.html
     
     class FirstprojectPipeline(object):
         def process_item(self, item, spider):
             return item
     ```

   * settings.py

     ```python
     # -*- coding: utf-8 -*-
     
     # Scrapy settings for firstproject project
     
     # For simplicity, this file contains only settings considered important or
     # commonly used. You can find more settings consulting the documentation:
     #     https://doc.scrapy.org/en/latest/topics/settings.html
     #     https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
     #     https://doc.scrapy.org/en/latest/topics/spider-middleware.html
     
     BOT_NAME = 'firstproject'
     
     SPIDER_MODULES = ['firstproject.spiders']
     NEWSPIDER_MODULE = 'firstproject.spiders' #爬虫文件的未知
     
     
     # Crawl responsibly by identifying yourself (and your website) on the user-agent
     #USER_AGENT = 'firstproject (+http://www.yourdomain.com)'  #User-Agent
     
     # Obey robots.txt rules
     ROBOTSTXT_OBEY = True  #是否检查ROBOTST规则，可以爬或者不可以爬
     
     # Configure maximum concurrent requests performed by Scrapy (default: 16)
     #CONCURRENT_REQUESTS = 32 #同时的并发请求，默认16
     
     # Configure a delay for requests for the same website (default: 0)
     # See https://doc.scrapy.org/en/latest/topics/settings.html#download-delay
     # See also autothrottle settings and docs
     #DOWNLOAD_DELAY = 3   #设置下载的时间间隔=》发送请求的间隔
     # The download delay setting will honor only one of:
     #CONCURRENT_REQUESTS_PER_DOMAIN = 16 #同一个域名并发的请求
     #CONCURRENT_REQUESTS_PER_IP = 16  #同一个IP并发的请求
     
     # Disable cookies (enabled by default)
     #COOKIES_ENABLED = False  # 是否打开cookie=》保证是一次会话
     
     # Disable Telnet Console (enabled by default)
     #TELNETCONSOLE_ENABLED = False
     
     # Override the default request headers:
     # 设置默认的请求头。。。
     #DEFAULT_REQUEST_HEADERS = {
     #   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
     #   'Accept-Language': 'en',
     #}
     
     # Enable or disable spider middlewares
     # See https://doc.scrapy.org/en/latest/topics/spider-middleware.html
     #SPIDER_MIDDLEWARES = {
     #    'firstproject.middlewares.FirstprojectSpiderMiddleware': 543,
     #}
     
     # Enable or disable downloader middlewares
     # See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html
     # 开启MIDDLEWARES
     #DOWNLOADER_MIDDLEWARES = {
     #    'firstproject.middlewares.FirstprojectDownloaderMiddleware': 543,
     #}
     
     # Enable or disable extensions
     # See https://doc.scrapy.org/en/latest/topics/extensions.html
     #EXTENSIONS = {
     #    'scrapy.extensions.telnet.TelnetConsole': None,
     #}
     
     # Configure item pipelines
     # See https://doc.scrapy.org/en/latest/topics/item-pipeline.html
     #ITEM_PIPELINES = {
     #    'firstproject.pipelines.FirstprojectPipeline': 300,
     #}
     
     # Enable and configure the AutoThrottle extension (disabled by default)
     # See https://doc.scrapy.org/en/latest/topics/autothrottle.html
     #AUTOTHROTTLE_ENABLED = True
     # The initial download delay
     #AUTOTHROTTLE_START_DELAY = 5
     # The maximum download delay to be set in case of high latencies
     #AUTOTHROTTLE_MAX_DELAY = 60
     # The average number of requests Scrapy should be sending in parallel to
     # each remote server
     #AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0
     # Enable showing throttling stats for every response received:
     #AUTOTHROTTLE_DEBUG = False
     
     # Enable and configure HTTP caching (disabled by default)
     # See https://doc.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings
     #HTTPCACHE_ENABLED = True
     #HTTPCACHE_EXPIRATION_SECS = 0
     #HTTPCACHE_DIR = 'httpcache'
     #HTTPCACHE_IGNORE_HTTP_CODES = []
     #HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'
     ```

3. 要改动的内容：

   * setting.py

     ```python
     #19行
     USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3710.0 Safari/537.36
     '  #User-Agent
     ```

